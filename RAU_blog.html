<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Enrique Nueve Blog</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Header -->
	<section id="header">
		<header>
			<span class="image avatar"><img src="images/me_pic.jpg" alt="" /></span>
			<h1 id="logo"><a href="index.html">Enrique Nueve</a></h1>
			<p>Undergraduate Statistics Major and Machine Learning Researcher.<br />
				Aspring Quantitative Analyst.</p>
		</header>
		<nav id="nav">
			<ul>
				<li><a href="index.html" class="active">Home</a></li>

			</ul>
		</nav>
		<footer>
			<ul class="icons">
				<li><a href="https://github.com/RealestRick-C137" class="icon brands fa-github"><span class="label">Github</span></a></li>
				<li><a href="https://www.linkedin.com/in/enrique-nueve-7a050115b/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
			</ul>
		</footer>
	</section>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">

			<!-- One -->
			<section id="one">
				<div class="image main" data-position="center">
					<img src="images/chicago_banner.jpg" alt="" />
				</div>
				<div class="container">
					<header class="major">
						<h2> IN PROGRESS </h2>
						<h2>Creating a Recurrent Attention Unit from scratch</h2>
						<p> August 2019 </p>
					</header>

					<h5> Intro </h5>
					<p> &emsp; This project was my first full scale Machine Learning model implementation. I collected the data, cleaned the data,
						built the model from scratch, ran test, and wrote an academic paper. Before this project, I had almost no experiance
						with Deep Learing models besides working with a MLP on a generic data set such as MNIST or Iris. I've never had formal teaching
						in machine learning, even today I've never taken a class. Even so, I was ambitious and wanted to become fluent in Deep Learning.
						This was my junior year in college and I had just transfered from Community College and watned to emerse myself in the new envioerment.
						I though upon transfering from Community College, I would then be able to take classes on Machine Learning and work with proffesors who were
						experianced in Machine Learning. With that in mind, I signed up for a research program for transfer students who had no research experiance.
						To my supresise, it was left up to the students to find a proffesor to work with. Even more to more surprise, the three professors
						at the University who worked in Machine Learning were all on sabatical. I knew I wanted to get into Machine Learning, and decided
						just to teach myself. How hard could it be?
					</p>
					<p></p>
					<h5> 1. Choosing a Topic </h5>
					<p> &emsp; I was now enrolled in this research program with no guidance. I did find a proffesor to work with yet, his focus was
						in Data Mining, and had no experiance in Deep Learning. Although he provided much insight into the struture of research, in regard
						to Deep Learning, he wasn't able to assit me. Even so, I wanted to be a Machine Learning researcher and being able to create and use
						Deep Learning models is the backbone of Machine Learning. At the time, I thought I understood Neural Networks fairly well. I did happen
						to build a Neural Network from scratch in Numpy (with the help of some tutorials...). So I felt faily confident I could pull this
						project off yet, I didn't know at the time what the project was going to be.
						<br></br>
						&emsp; From my limited exposure, I knew only a handful of topics and concepts. Ovreall, I knew I wanted to be a Qunatitivae Analysit and
						I wanted to do a Deep Learning project. Put one and one together, and after a quick couple of Googles, you realize why don't I do a Time
						Series forcasting project (as I mentioned, this was my first large scale Machine Learning project). I was conciveced it was going to be
						a breeze and since, I was using "Deep Leanring" it was going to be amazingly accurate. Even though I had no clue what was novel about
						this project at the time, I brushed it off and decided a idea would come to me during the project.
					</p>
					<p></p>
					<h5> 2. Tensors? What does tension have to with Deep Learning? </h5>
					<p> &emsp; Turns out tensors and tension have nothing do with each other, this was shocking. However, not as shocking as
						having the realization that majority of Deep Learning post on the internet have no clue about the mathmatics of
						backpropogation of networks. After a couple of weeks of pondering what to do for my project, I decided to build a
						Reccurent Network to perform Time Series forcasting. Also, I decided I was going to built it only from Numpy. That means that
						I had to perform the deviations for foward passing, backpropogation, and gradient updating by hand. After several weeks of no luck, I
						decided to look up some tutorials on the mathmatics of backpropogation. Quickly, I noticed theses turiaals almost never showed any mathmatics
						but were just a discuassion of the concept of backpropogation. I truged on and by hand I could derive a Neural Network with all of its
						processes. I tested my deviations by building a new Neural Network from scratch (no tutorials) and was able to get it to work. Now I decided to
						go back and look at some deviatons for Reccurent Networks. This time, less supressed, the deviations at best only showed how to perform
						foward passes but never backpropogation for Reccurrent networks. So it was back to the drawing board.
					</p>
					<p> &emsp; After relentless testing, I found that the mathmatics for backpropogation just did not add up to how Reccruent networks
						perform foward passing. This led me to realize that yes, standard backpropogation does not work on Reccruent network. Although
						this may seem obvious if you have taken classes or had a book, neither which I had, this was absolutely shocking since no blog post or video
						I watched ever mentioned this. Turns out, there is a varient of backpropogation called Backpropagation Through Time, BPTT. Upon discovering this,
						my first thought was, since I was building a network from scratch, how do I derive this? Again, I learned the hard lesson of wanting
						devitions from Compute Scientist is a loss cause. After several weeks of reading about Tensor Operationa and Tensor Calclus I figured out the deviations.
						I coded the network and got it working on forcasting time series stock data.
					</p>
					<p></p>
					<h5> 3. Mankind was never intended to take the Jacobian of a Kronecker Product </h5>
					<p> &emsp; Looking back, this might have been the most antagonzing deviation I have ever attempted.
						Upon working through the deviations of a Recurrent Network and specifically BPTT, I was ready for my
						next challange. I was ready to take on a Recurrent Network with a state space, the LSTM. As I was doing
						all the coding through Numpy, I had to perform the deviations myself. For the first time, I was introduced
						to the Kronecker Product. Although this operatin isn't that complicated to compute,it causes the dimensionality
						of the output to increase by the multiplied values of both matrix's dimension size of their rows. Howeverv,
						the true issue was that by taking the jacobian of this output, the size grew once again. So how do you
						perform backpropogation in respect to a weight that is nowhere near the dimension size of the
						output of the Kronecker product which occurs as an operation in the LSTM. I did eventually figure
						out the mathmatics yet, only for a vector input. Even so, nowhere has anyone ever posted the full
						deviations for when you input a training sample in the form of a batch in the shape of a matrix. Although
						today I do know how to compute this through a method of broadcasting and averaging out the backpropogated errors,
						at the time I just couldn't figure it out. This led me to the much more computationally friendly GRU.
					</p>
					<p></p>
					<h5> 4. Trust in the Multivariable Chain Rule </h5>
					<p> &emsp; I was estatic to discover the Gated Reccruent Unit, GRU. The mathmatics
						were fairly straight foward. Backpropagation for the network I was smoothly able to
						derive and then code the network. I had the network working and was quite satisfied, till
						I wasn't. The mathmatics for how the state network and input network combined just made
						me feel off. How could combining two networks, with completly different data entered, be combined
						and still make a logical network that could properly model. This led me to researching
						the much older file of state space models. This led me to understand how the state could
						serve as a map to intrpet which event is occuring and develop a "sudo mememory." This question
						also led me to further understand Hilbert Spaces and how backpropogation can truly lead
						to the construction of best approxoimations of Inner Product Spaces. This also led me to
						the most unappreacited mathamtical proofs that every person who works with Machine Learning to
						know about, "The Universal Approxoimation Theorem." Finanly, I had a working model yet, a question
						came back to my mind, how is the project unique?
					</p>
					<p></p>
					<h5> 5. I was convinced having orginal ideas were impossible </h5>
					<p> &emsp; Being that this was my first offical research project, I never had experianced the
						neccesity to have an original idea on command. This task seemed so unatural. There
						are no guide books to have an original idea. I struggled for weeks to come up with a way
						to make my project unique from any other Time Series Forcasting project. I had a clock running
						out of time and a lack of experiance to match. However, I did learn about how to come up with
						original ideas. I learned to stick with you intrpetation of things to have orginal ideas.
						I taught myself everything about Deep Learning on my own. No feedback from anyone except
						the mathmatics and the code of the models. No explnation ever why anything worked, I only
						had definte proof from testing which left me to figure out the "why's" of Deep Learning. I
						learned to question everything, be vicious to read about the topic I wasn't familer with,
						and to not settle for unexplained reasons for anything that I built (Cough cough Tensorflow).
						Although I did not have this clarity during the time I was looking for an original idea,
						my perspective was always with me which led me to an original idea.
					</p>
					<p> &emsp; As I was frantically reading paper after paper for insperation, I came upon a paper
						on adaptive activation functions. I ran upon a paper for a noisy activation function. It seemed
						logical enough however, I noticed a a discpency between their outlook on activation functions
						and mine. From working by the deivations and proofs of convegency for Neural Networks, I have
						a strong belief about the necesity of activation functions to have nonlinear attributes to be able
						to properly approximate functions. Even so, this paper proposed an acitvation function modification
						to sigmoid in which gausian noise was added for outlieng values. This caused anyvalues not along the linear
						shaped center of the function to have noise added. This makes sense for the purpose of serving as a regressor
						however, with my undetstaning of how it is neccesasry for activation functions to have nonlinear
						features, I decided I would make my own noisy activation function with nonlinear features.
						This led to my original activation function the Noisy Exponential Activation Function, NEAF.
					</p>
					<p></p>
					<h5> 6. Noisy Exponential Activation Function, NEAF </h5>

					<figure>
						<img src="RAU_images/NEAF_graph.png" class="image fit" alt="NEAF graph" />
						<figcaption> Visual of NEAF </figcaption>
					</figure>

					<p></p>

					<pre><code>
	import numpy as np

	def NEAF(x, deriv=False):
	    def noise(x):
	        return np.abs(np.random.randn(np.size(x)))

	    def linear(x, deriv=False):
	        if deriv:
	            return 0.25
	        return 0.25*x + 0.5

	    def hard_sigmoid(x, deriv=False):
	        if deriv:
	            x = np.piecewise(x,[x >= 2, (-2<x) & (x<2) ,x <= -2],[lambda x: 0, lambda x: .25 ,lambda x: 0])
	            return x
	        return np.maximum(0, np.minimum(1, (x + 2) / 4))

	    if deriv:
	        x = linear(x)
	        x = np.piecewise(x,[x > .975, (-.025<=x) & (x<=.975) ,x <= .025],
	        [lambda x: -.01*noise(x)*(1-hard_sigmoid(x,deriv=True))*np.exp(x-hard_sigmoid(x)),
	        lambda x: 1 ,
	        lambda x: .01*noise(x)*(hard_sigmoid(x,deriv=True)-1)*np.exp(hard_sigmoid(x)-x)])
	        x = x*.25
	        return x

	    x = linear(x)
	    x = np.piecewise(x ,[x < 0.025, x > 0.025],[lambda x: 0.01 * np.exp(hard_sigmoid(x) - x) * noise(x), lambda x: x])
	    x = np.piecewise(x ,[x > 0.975, x < 0.975],[lambda x: 1 - 0.01 * np.exp(x-hard_sigmoid(x)) * noise(x), lambda x: x])
	    return x

</code></pre>

					<p></p>

					<figure>
						<img src="RAU_images/NEAF_formula.png" class="image fit" alt="NEAF formula" />
						<figcaption> NEAF formula </figcaption>
					</figure>






					<p>
						<h5> Citations </h5>
						[1] Zhong, Guoqiang, Guohua Yue, and Xiao Ling. "Recurrent Attention Unit." arXiv preprint arXiv:1810.12754 (2018).
					</p>


				</div>
			</section>




		</div>

		<!-- Footer -->
		<section id="footer">
			<div class="container">
				<ul class="copyright">
					<li>&copy; Untitled. All rights reserved.</li>
					<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</section>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>
