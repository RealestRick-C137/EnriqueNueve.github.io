<!DOCTYPE HTML>
<!--
   Read Only by HTML5 UP
   html5up.net | @ajlkn
   Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
   -->
<html>

<head>
  <title>Enrique Nueve Blog</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">
  <!-- Header -->
  <section id="header">
    <header>
      <span class="image avatar"><img src="images/me_pic.jpg" alt="" /></span>
      <h1 id="logo"><a href="index.html">Enrique Nueve</a></h1>
      <p>ML Engineer and Aspiring Researcher</p>
    </header>
    <nav id="nav">
      <ul>
        <li><a href="index.html" class="active">Home</a></li>
      </ul>
    </nav>
    <footer>
      <ul class="icons">
        <li><a href="https://github.com/EnriqueNueve" class="icon brands fa-github"><span class="label">Github</span></a></li>
        <li><a href="https://www.linkedin.com/in/enrique-nueve-7a050115b/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
      </ul>
    </footer>
  </section>
  <!-- Wrapper -->
  <div id="wrapper">
    <!-- Main -->
    <div id="main">
      <!-- One -->
      <section id="one">
        <div class="image main" data-position="center">
          <img src="images/chicago_banner.jpg" alt="" />
        </div>
        <div class="container">
          <header class="major">
            <h2>Under Construction</h2>
            <h2>Semantic Segmentation</h2>
            <p> Winter 2020</p>
          </header>
          <br></br>

          <h3>1. Intro</h3>
          <p> &emsp; Deep Learning has many uses within Computer Vision, such as classification, filling,
            optical flow, object detection, and semantic segmentation. In this post, I will be presenting
            an overview of the task of semantic segmentation and how I went about creating a Deep Learning
            model, specifically a Fully Convolutional Network with transfer learning over a ResNet, with
            TensorFlow 2 to perform semantic segmentation.
          </p>
          <p></p>

          <h3>2. Semantic Segmentation</h3>
          <p> &emsp; A common first exercise for Deep Learning is to make a model perform image classification.
            This includes building a model that can
            take in an image and then output a one-hot vector, which
            represents the model's belief of what type of object is within the image.
            Although in practice this could be very useful, in some instances, it would
            also, be informative to know where in the image the believed object is located.
            For example, say you created a model to classify whether a boat is stranded in the ocean
            based on satellite imagery. If the model alerted the Coast Guard that there was a
            stranded boat but didn't show where the boat was located would, it would make it
            infeasible to act upon such knowledge. Semantic segmentation addresses such a dilemma.
          </p>
          <p> &emsp; Semantic segmentation is the task of classifying each pixel in an image. Instead
            of just saying whether a class is present in an image, it will try to predict which pixels
            within the image belong to said class. Image segmentation is inherently more informative
            to a practitioner than plain image classification, yet the task is more challenging in
            several ways.
          </p>
          <p> &emsp; Since semantic segmentation is trying to predict the class of each pixel, the
            training data requires that each pixel be classified within an image. This causes the task of creating a
            training set to be a grueling process. To counter such challenges of making a dataset,
            different tools and paid services exist that will either assist or perform data labeling.
            Beyond the challenges of creating a dataset, it is also challenging to train a model given
            that the size of the data is rather large. Each target value consists of a 2D matrix where
            the entries correspond to a number that corresponds to the class the pixel belongs to. The
            size of the data makes it necessary in most cases to use data loaders and optimized
            file types for reading and writing, such as tf.Records from TensorFlow 2. Finally, it can be rather
            challenging for Deep Learning models to converge. Given that
            each pixel contributes an error value; the model can easily have the gradient
            explosion or collapse occur.
          </p>


          <p></p>

          <h3>3. Fully Convolutional Network (FCN)</h3>
          <p> &emsp; To perform semantic segmentation, I went about using a model called a Fully Convolutional Network (FCN) [1].
            FCNs take in as input an image; this could be
            a grayscale (width x height x 1) or color (width x height x 3) image, and then outputs
            a 3D tensor (width x height x class) which expresses the prediction of each pixel's respective
            class. This is done by first sending the input image through a series of convolutional layers and
            then through a series of convolutional transposed layers that upscale the dimensionality of data
            back to the original width and height of the input but with a depth of the number of target classes.
            For those unfamiliar with the idea of convolutional transposed layers, it may be confusing how you could
            increase the dimensionality of the output of a convolutional layer. To help build some intuition to how
            convolutional transposed layers work, let's go over a numerical example.
          </p>
          <figure>
            <img src="images/fcn_diagram.jpg" class="image fit" />
            <figcaption> FCN diagram from [1]</figcaption>
          </figure>

          <h4>3a. Convolutions</h4>
          <p>
            &emsp;At the beginning of the FCN, you initially pass an input image through a series of Convolutional layers.
            Each of the Convolutional layers have kernels which are trainable parameters. Intuitively, the kernel scans across each
            row of the input image and outputs a value from each distinct position the kernel passes over. Although
            the layer is called a Convolutional layer, the operation is actually the cross-correlation function. The cross-correlation
            and a convolution act the same within a neural network due to trainable parameters; however,
            from a purely mathematical definition, they are different.
            To learn more about how convolutions are used for Convolutional networks, I would refer
            to this <a href="https://www.deeplearningbook.org/contents/convnets.html">chapter</a> from the book
            called Deep Learning (yes, the name of the book is Deep Learning). The cross-correlation function
            is expressed by the formula below.
          </p>
          <figure>
            <img src="images/cross_corr_formula.png" class="image fit" />
            <figcaption> Convolution (Cross-Correlation) formula</figcaption>
          </figure>
          The formula expresses how to compute a value for a single location, S(i,j), between K, the kernel, and I,
          the input value. In essence, the formula states to multiply each value of the kernel with the value of
          the input in which it overlaps. Then, sum up all of the computed values between the kernel and the distinct
          location on the input. This total value expresses the output for a location between the kernel and input.
          Although somewhat abstract, this idea can be further expressed by taking a
          look at the visual below, which shows a cross-correlation operation between a kernel and an input value.
          Between using the formula and the diagram below, try to see how the outputted values are constructed.
          Overall, there are many technicalities when implementing a Convluitional network, and by no means does
          this summarize the operation. This except is just to provide intuition in order to understand the FCN.
          As before, I recommend reading
          the following <a href="https://www.deeplearningbook.org/contents/convnets.html">chapter</a> from the book
          called Deep Learning.
          <figure>
            <img src="images/conv_pic.png" class="image fit" />
            <figcaption> Convolution (Cross-Correlation) example</figcaption>
          </figure>



          <h4>3b. Transposed Convolutions</h4>
          Through using Convolutional layers, the model is able to shrink the input but, also, like any other neural network,
          is able to perform feature engineering on the input as it goes through layers. Recall, the kernel from the
          convolutional layers are trainable parameters. Therefore, after passing the input through a series of Convultinal Layers, a 
          new smaller feature enhanced representation exist. This enhanced feature representation, the output from the
          Convolutional layers is sometimes referred to as a Feature Map within Deep Learning Computer Vison literature. However, since
          our task is semantic segmentation, which means we want to output a grid of the same size as our input where each value
          in the grid corresponds to the class of each pixel from the original input image, we have an issue since our new
          representation of the data is now smaller than it originally was.

          <figure>
            <img src="images/trans_conv_pic.png" class="image fit" />
            <figcaption> Transposed Convolution example </figcaption>
          </figure>
          <p>

          </p>


          <h3>4. VOC 2012 Dataset</h3>
          <p> &emsp; The start.
          </p>
          <p></p>

          <h3>5. Implementation</h3>
          <p> &emsp; The start.
          </p>
          <figure>
            <img src="images/sample_pred_fcn.png" class="image fit" />
            <figcaption> Example of predicted mask (Thats my dog Mocha)</figcaption>
          </figure>
          <p></p>

          <h3>6. Conclusion</h3>
          <p>
            The end.
          </p>

          <p>
            <h5> Citations </h5>
            [1] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation."
            Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.
          </p>


        </div>
      </section>
    </div>
    <!-- Footer -->
    <section id="footer">
      <div class="container">
        <ul class="copyright">
          <li>&copy; Untitled. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
        </ul>
      </div>
    </section>
  </div>
  <!-- Scripts -->
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/jquery.scrollex.min.js"></script>
  <script src="assets/js/jquery.scrolly.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>
</body>

</html>
